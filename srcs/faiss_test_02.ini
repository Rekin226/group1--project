
"""
Updated FAISS script using Dictionary_02.py for data integration.
"""

from langchain_huggingface import HuggingFaceEmbeddings
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from uuid import uuid4
from langchain_core.documents import Document
import pandas as pd
import Dictionary_02 as d2
import requests
from bs4 import BeautifulSoup

# 取得 Dictionary_02.py 中的資料表
df = d2.df

# 動態提取 URL 中的內容
def extract_text_from_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text()
    except Exception as e:
        print(f"Error fetching data from URL {url}: {e}")
        return ""

# 添加從 URL 提取的文本到 DataFrame
df["extracted_content"] = df["source"].apply(extract_text_from_url)

# 使用 HuggingFace 嵌入模型
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
embedding_dim = len(embeddings.embed_query("hello world"))
index = faiss.IndexFlatL2(embedding_dim)

# 初始化向量存儲
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

# 創建 Document 對象，包含提取的文本與元數據
documents = [
    Document(page_content=row["extracted_content"] or row["content"], 
             metadata={"source": row["source"]})
    for _, row in df.iterrows()
]

# 打印文檔數量
print(f"Added {len(documents)} documents to the vector store.")

# 為文檔生成唯一 ID 並添加到向量存儲
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)

# 打印向量存儲大小
print(f"FAISS index size: {index.ntotal}")

# 進行相似性搜索
query = "Plant Care"
results = vector_store.similarity_search(query)
print(f"Search results: {results}")

# 帶分數的相似性搜索
results_with_scores = vector_store.similarity_search_with_score(
    "What is nutrient levels?", k=1
)

# 打印搜索結果
for res, score in results_with_scores:
    print(f"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]")
